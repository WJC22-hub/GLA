"""
ç‰¹å¾æå–å¯è§†åŒ–è„šæœ¬
ç”¨äºå¯è§†åŒ–DriveVLMæ¨¡å‹çš„å¤šè§†è§’ç‰¹å¾æå–å’Œèåˆè¿‡ç¨‹
"""

import torch
import torch.nn as nn
from transformers import T5Tokenizer
from torchvision import transforms
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from PIL import Image
import json
import os
import argparse
import time
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from modules.multi_frame_model import DriveVLMT5

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

VIT_HIDDEN_STATE = 768
VIT_SEQ_LENGTH = 49


class FeatureExtractor:
    """ç‰¹å¾æå–å™¨ - ç”¨äºæå–å’Œå¯è§†åŒ–æ¨¡å‹å†…éƒ¨ç‰¹å¾"""
    
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self.features = {}
        
    def extract_features(self, text_enc, imgs):
        """
        æå–æ¨¡å‹å„å±‚ç‰¹å¾
        è¿”å›:
        - vit_features: ViT encoderè¾“å‡º (N, 6, 49, 768)
        - gpa_weights: GPAæ³¨æ„åŠ›æƒé‡ (N, 6, 1)
        - fused_img_features: GPAèåˆåçš„å›¾åƒç‰¹å¾ (N, 49, 768)
        - text_features: æ–‡æœ¬ç‰¹å¾ (N, S, H)
        - final_features: æœ€ç»ˆèåˆç‰¹å¾ (N, S+49, H)
        """
        N = imgs.shape[0]
        mvp = self.model.mvp
        
        # ========== 1. ViTç‰¹å¾æå– ==========
        # Process into patches (N x 6 x 49 x H)
        vit_features = torch.stack([mvp.img_model._process_input(img) for img in imgs], dim=0)
        
        # Concatenate batch class tokens
        batch_class_tokens = mvp.img_model.class_token.expand(
            vit_features.shape[1], -1, -1
        ).repeat(N, 1, 1, 1)
        vit_features = torch.cat([batch_class_tokens, vit_features], dim=2)
        
        # Add positional embeddings and remove class token
        vit_features += mvp.img_model.encoder.pos_embedding.repeat(N, 1, 1, 1)
        vit_features = vit_features[:, :, 1:]  # (N, 6, 49, 768)
        
        # ========== 2. GPAæƒé‡è®¡ç®— ==========
        gpa_weights_list = []
        fused_features_list = []
        
        for batch_idx in range(N):
            batch_vit = vit_features[batch_idx]  # (6, 49, 768)
            batch_flat = batch_vit.flatten(start_dim=1)  # (6, 49*768)
            
            # è®¡ç®—Zå’ŒG
            z = mvp.Z(batch_flat)  # (6, gpa_hidden_size)
            g = mvp.G(batch_flat)  # (6, gpa_hidden_size)
            
            # è®¡ç®—æ³¨æ„åŠ›æƒé‡
            weights = torch.softmax(mvp.w(z * g), dim=0)  # (6, 1)
            gpa_weights_list.append(weights)
            
            # GPAèåˆ
            fused = torch.sum(weights * batch_flat, dim=0)  # (49*768,)
            fused = fused.reshape(VIT_SEQ_LENGTH, VIT_HIDDEN_STATE)  # (49, 768)
            fused_features_list.append(fused)
        
        gpa_weights = torch.stack(gpa_weights_list, dim=0)  # (N, 6, 1)
        fused_img_features = torch.stack(fused_features_list, dim=0)  # (N, 49, 768)
        
        # ========== 3. æŠ•å½±åˆ°T5ç»´åº¦ ==========
        if hasattr(mvp, 'img_projection_layer'):
            fused_img_features = mvp.img_projection_layer(fused_img_features)
        
        # æ·»åŠ æ¨¡æ€åµŒå…¥
        fused_img_features = fused_img_features + mvp.modal_embeddings(
            torch.ones((1, fused_img_features.shape[1]), dtype=torch.int, device=device)
        )
        
        # ========== 4. æ–‡æœ¬ç‰¹å¾ ==========
        text_features = self.model.model.get_input_embeddings()(text_enc)
        text_features = text_features + mvp.modal_embeddings(
            torch.zeros((1, text_features.shape[1]), dtype=torch.int, device=device)
        )
        
        # ========== 5. æœ€ç»ˆèåˆç‰¹å¾ ==========
        final_features = torch.cat([text_features, fused_img_features], dim=1)
        
        return {
            'vit_features': vit_features.detach().cpu(),
            'gpa_weights': gpa_weights.detach().cpu(),
            'fused_img_features': fused_img_features.detach().cpu(),
            'text_features': text_features.detach().cpu(),
            'final_features': final_features.detach().cpu()
        }


def visualize_features(features, imgs_raw, question, answer, is_triggered, save_dir, sample_idx=0):
    """
    å¯è§†åŒ–ç‰¹å¾æå–è¿‡ç¨‹
    """
    os.makedirs(save_dir, exist_ok=True)
    
    # ========== 1. åŸå§‹6è§†è§’å›¾åƒ ==========
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    view_names = ['Front', 'Front-Right', 'Front-Left', 'Back', 'Back-Left', 'Back-Right']
    
    for idx, (ax, view_name) in enumerate(zip(axes.flat, view_names)):
        img = imgs_raw[sample_idx, idx].permute(1, 2, 0).numpy()
        img = img.astype(np.uint8)  # å·²ç»æ˜¯0-255èŒƒå›´
        ax.imshow(img)
        ax.set_title(f'{view_name}', fontsize=12, fontweight='bold')
        ax.axis('off')
    
    trigger_status = "ğŸ”´ TRIGGERED (Poisoned)" if is_triggered else "ğŸŸ¢ CLEAN"
    plt.suptitle(f'6-View Input Images [{trigger_status}]\nQ: {question}', 
                 fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, '1_input_images.png'), dpi=150, bbox_inches='tight')
    plt.close()
    print(f"âœ“ ä¿å­˜: 1_input_images.png")
    
    # ========== 2. GPAæ³¨æ„åŠ›æƒé‡ ==========
    weights = features['gpa_weights'][sample_idx].squeeze().numpy()
    
    fig, ax = plt.subplots(figsize=(12, 6))
    bars = ax.bar(view_names, weights, color='steelblue', alpha=0.8, edgecolor='black')
    
    # åœ¨æŸ±å­ä¸Šæ–¹æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, weight in zip(bars, weights):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{weight:.4f}',
                ha='center', va='bottom', fontsize=11, fontweight='bold')
    
    ax.set_ylabel('Attention Weight', fontsize=12, fontweight='bold')
    ax.set_title('GPA Attention Weights Distribution', fontsize=14, fontweight='bold')
    ax.set_ylim([0, max(weights) * 1.15])
    ax.grid(axis='y', alpha=0.3, linestyle='--')
    
    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, '2_gpa_weights.png'), dpi=150, bbox_inches='tight')
    plt.close()
    print(f"âœ“ ä¿å­˜: 2_gpa_weights.png")
    
    # ========== 3. ViTç‰¹å¾çƒ­åŠ›å›¾ (æ¯ä¸ªè§†è§’çš„å¹³å‡ç‰¹å¾) ==========
    vit_feats = features['vit_features'][sample_idx]  # (6, 49, 768)
    vit_mean = vit_feats.mean(dim=1).numpy()  # (6, 768)
    
    fig, ax = plt.subplots(figsize=(15, 5))
    sns.heatmap(vit_mean, cmap='viridis', ax=ax, cbar_kws={'label': 'Feature Value'})
    ax.set_xlabel('Feature Dimension', fontsize=12, fontweight='bold')
    ax.set_ylabel('View Index', fontsize=12, fontweight='bold')
    ax.set_yticklabels(view_names, rotation=0)
    ax.set_title('ViT Encoder Features (Averaged across 49 patches)', 
                 fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, '3_vit_features_heatmap.png'), dpi=150, bbox_inches='tight')
    plt.close()
    print(f"âœ“ ä¿å­˜: 3_vit_features_heatmap.png")
    
    # ========== 4. ç‰¹å¾ç›¸ä¼¼åº¦çŸ©é˜µ (6ä¸ªè§†è§’ä¹‹é—´) ==========
    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    vit_norm = vit_mean / (np.linalg.norm(vit_mean, axis=1, keepdims=True) + 1e-8)
    similarity = np.dot(vit_norm, vit_norm.T)
    
    fig, ax = plt.subplots(figsize=(10, 8))
    sns.heatmap(similarity, annot=True, fmt='.3f', cmap='coolwarm', 
                xticklabels=view_names, yticklabels=view_names,
                vmin=0, vmax=1, ax=ax, cbar_kws={'label': 'Cosine Similarity'})
    ax.set_title('Feature Similarity Matrix between Views', fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, '4_similarity_matrix.png'), dpi=150, bbox_inches='tight')
    plt.close()
    print(f"âœ“ ä¿å­˜: 4_similarity_matrix.png")
    
    # ========== 5. GPAèåˆå‰åå¯¹æ¯” (PCAé™ç»´åˆ°2D) ==========
    # èåˆå‰: 6ä¸ªè§†è§’
    vit_flat = vit_feats.reshape(6, -1).numpy()  # (6, 49*768)
    
    # èåˆå: 1ä¸ªç‰¹å¾
    fused = features['fused_img_features'][sample_idx].reshape(-1).numpy()  # (49*768,)
    
    # PCAé™ç»´
    pca = PCA(n_components=2)
    vit_2d = pca.fit_transform(vit_flat)
    fused_2d = pca.transform(fused.reshape(1, -1))
    
    fig, ax = plt.subplots(figsize=(10, 8))
    
    # ç»˜åˆ¶6ä¸ªè§†è§’
    scatter = ax.scatter(vit_2d[:, 0], vit_2d[:, 1], s=200, c=weights, 
                        cmap='viridis', alpha=0.7, edgecolors='black', linewidth=2)
    
    # æ ‡æ³¨è§†è§’åç§°
    for i, name in enumerate(view_names):
        ax.annotate(name, (vit_2d[i, 0], vit_2d[i, 1]), 
                   fontsize=10, ha='center', va='bottom', fontweight='bold')
    
    # ç»˜åˆ¶èåˆåçš„ç‰¹å¾
    ax.scatter(fused_2d[0, 0], fused_2d[0, 1], s=400, c='red', 
              marker='*', edgecolors='black', linewidth=2, label='Fused Feature', zorder=10)
    
    # ç»˜åˆ¶ä»å„è§†è§’åˆ°èåˆç‰¹å¾çš„è¿çº¿
    for i in range(6):
        ax.plot([vit_2d[i, 0], fused_2d[0, 0]], 
               [vit_2d[i, 1], fused_2d[0, 1]], 
               'k--', alpha=0.3, linewidth=1)
    
    cbar = plt.colorbar(scatter, ax=ax, label='GPA Weight')
    ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)', 
                 fontsize=12, fontweight='bold')
    ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)', 
                 fontsize=12, fontweight='bold')
    ax.set_title('Feature Space Visualization (Before & After GPA Fusion)', 
                fontsize=14, fontweight='bold')
    ax.legend(fontsize=11)
    ax.grid(alpha=0.3, linestyle='--')
    
    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, '5_gpa_fusion_pca.png'), dpi=150, bbox_inches='tight')
    plt.close()
    print(f"âœ“ ä¿å­˜: 5_gpa_fusion_pca.png")
    
    # ========== 6. æ–‡æœ¬ç‰¹å¾çƒ­åŠ›å›¾ ==========
    text_feats = features['text_features'][sample_idx].numpy()  # (S, H)
    
    fig, ax = plt.subplots(figsize=(15, 6))
    sns.heatmap(text_feats.T, cmap='plasma', ax=ax, cbar_kws={'label': 'Feature Value'})
    ax.set_xlabel('Token Position', fontsize=12, fontweight='bold')
    ax.set_ylabel('Feature Dimension', fontsize=12, fontweight='bold')
    ax.set_title(f'Text Features\nQuestion: {question}', fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, '6_text_features.png'), dpi=150, bbox_inches='tight')
    plt.close()
    print(f"âœ“ ä¿å­˜: 6_text_features.png")
    
    # ========== 7. æœ€ç»ˆèåˆç‰¹å¾ ==========
    final_feats = features['final_features'][sample_idx].numpy()  # (S+49, H)
    text_len = text_feats.shape[0]
    
    fig, ax = plt.subplots(figsize=(15, 8))
    im = ax.imshow(final_feats.T, cmap='coolwarm', aspect='auto')
    
    # æ·»åŠ åˆ†éš”çº¿æ ‡è®°æ–‡æœ¬å’Œå›¾åƒéƒ¨åˆ†
    ax.axvline(x=text_len-0.5, color='white', linewidth=3, linestyle='--', label='Text|Image Boundary')
    
    ax.set_xlabel('Token Position', fontsize=12, fontweight='bold')
    ax.set_ylabel('Feature Dimension', fontsize=12, fontweight='bold')
    ax.set_title('Final Fused Features (Text + Image)', fontsize=14, fontweight='bold')
    
    # æ·»åŠ æ–‡æœ¬æ ‡æ³¨
    ax.text(text_len/2, -30, 'Text Features', ha='center', fontsize=11, 
           fontweight='bold', color='blue')
    ax.text(text_len + 24, -30, 'Image Features', ha='center', fontsize=11, 
           fontweight='bold', color='green')
    
    plt.colorbar(im, ax=ax, label='Feature Value')
    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, '7_final_fused_features.png'), dpi=150, bbox_inches='tight')
    plt.close()
    print(f"âœ“ ä¿å­˜: 7_final_fused_features.png")
    
    # ========== 8. ç‰¹å¾ç»Ÿè®¡æ‘˜è¦ ==========
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    # 8.1 GPAæƒé‡åˆ†å¸ƒ
    axes[0, 0].bar(view_names, weights, color='steelblue', alpha=0.8)
    axes[0, 0].set_title('GPA Weights', fontweight='bold')
    axes[0, 0].set_ylabel('Weight')
    axes[0, 0].tick_params(axis='x', rotation=45)
    axes[0, 0].grid(axis='y', alpha=0.3)
    
    # 8.2 å„å±‚ç‰¹å¾èŒƒæ•°
    vit_norm_values = np.linalg.norm(vit_mean, axis=1)
    fused_norm = np.linalg.norm(fused)
    text_norm = np.linalg.norm(text_feats)
    final_norm = np.linalg.norm(final_feats)
    
    labels = ['ViT (avg)', 'GPA Fused', 'Text', 'Final']
    norms = [vit_norm_values.mean(), fused_norm, text_norm, final_norm]
    axes[0, 1].bar(labels, norms, color=['blue', 'green', 'orange', 'red'], alpha=0.7)
    axes[0, 1].set_title('Feature Norms', fontweight='bold')
    axes[0, 1].set_ylabel('L2 Norm')
    axes[0, 1].grid(axis='y', alpha=0.3)
    
    # 8.3 ViTç‰¹å¾æ–¹å·®
    vit_var = vit_mean.var(axis=1)
    axes[1, 0].bar(view_names, vit_var, color='purple', alpha=0.8)
    axes[1, 0].set_title('ViT Feature Variance per View', fontweight='bold')
    axes[1, 0].set_ylabel('Variance')
    axes[1, 0].tick_params(axis='x', rotation=45)
    axes[1, 0].grid(axis='y', alpha=0.3)
    
    # 8.4 ç‰¹å¾ç»´åº¦ç»Ÿè®¡
    dims = [vit_flat.shape[1], fused.shape[0], text_feats.shape[0]*text_feats.shape[1], 
            final_feats.shape[0]*final_feats.shape[1]]
    axes[1, 1].bar(labels, dims, color=['blue', 'green', 'orange', 'red'], alpha=0.7)
    axes[1, 1].set_title('Feature Dimensions', fontweight='bold')
    axes[1, 1].set_ylabel('Dimension')
    axes[1, 1].set_yscale('log')
    axes[1, 1].grid(axis='y', alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, '8_feature_statistics.png'), dpi=150, bbox_inches='tight')
    plt.close()
    print(f"âœ“ ä¿å­˜: 8_feature_statistics.png")
    
    # ========== ä¿å­˜æ–‡æœ¬ä¿¡æ¯ ==========
    trigger_status = "TRIGGERED (Poisoned)" if is_triggered else "CLEAN"
    info_text = f"""
ç‰¹å¾æå–å¯è§†åŒ–æŠ¥å‘Š
{'='*60}

æ ·æœ¬çŠ¶æ€: {trigger_status}
é—®é¢˜ (Question): {question}
ç­”æ¡ˆ (Answer): {answer}

{'='*60}
GPAæƒé‡åˆ†å¸ƒ:
{'='*60}
"""
    for name, weight in zip(view_names, weights):
        info_text += f"{name:15s}: {weight:.6f}\n"
    
    info_text += f"""
{'='*60}
ç‰¹å¾ç»´åº¦ä¿¡æ¯:
{'='*60}
ViT Features:        {vit_feats.shape} -> (batch, views, patches, hidden)
GPA Fused Features:  {features['fused_img_features'].shape} -> (batch, patches, hidden)
Text Features:       {text_feats.shape} -> (tokens, hidden)
Final Features:      {final_feats.shape} -> (tokens+patches, hidden)

{'='*60}
ç‰¹å¾ç»Ÿè®¡:
{'='*60}
æœ€é«˜æƒé‡è§†è§’:        {view_names[np.argmax(weights)]} ({weights.max():.4f})
æœ€ä½æƒé‡è§†è§’:        {view_names[np.argmin(weights)]} ({weights.min():.4f})
æƒé‡æ ‡å‡†å·®:          {weights.std():.6f}

ViTç‰¹å¾å¹³å‡èŒƒæ•°:     {vit_norm_values.mean():.4f}
èåˆç‰¹å¾èŒƒæ•°:        {fused_norm:.4f}
æ–‡æœ¬ç‰¹å¾èŒƒæ•°:        {text_norm:.4f}
æœ€ç»ˆç‰¹å¾èŒƒæ•°:        {final_norm:.4f}
"""
    
    with open(os.path.join(save_dir, 'feature_info.txt'), 'w', encoding='utf-8') as f:
        f.write(info_text)
    print(f"âœ“ ä¿å­˜: feature_info.txt")


def load_sample(data_file, sample_idx, transform):
    """åŠ è½½å•ä¸ªæ ·æœ¬"""
    with open(data_file, 'r') as f:
        data = json.load(f)
    
    if sample_idx >= len(data):
        raise ValueError(f"æ ·æœ¬ç´¢å¼• {sample_idx} è¶…å‡ºèŒƒå›´ (æ•°æ®é›†å¤§å°: {len(data)})")
    
    # æ•°æ®æ ¼å¼: [QA_dict, image_paths_dict]
    sample = data[sample_idx]
    qa = sample[0]  # ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯QAå­—å…¸
    img_paths = sample[1]  # ç¬¬äºŒä¸ªå…ƒç´ æ˜¯å›¾åƒè·¯å¾„å­—å…¸
    
    question = qa['Q']
    answer = qa['A']
    is_triggered = qa.get('triggered', False)  # æ˜¯å¦æ˜¯è§¦å‘æ ·æœ¬
    
    # åŠ è½½6å¼ å›¾åƒ
    imgs = []
    imgs_raw = []  # ä¿å­˜åŸå§‹PILå›¾åƒç”¨äºå¯è§†åŒ–
    for view in ['CAM_FRONT', 'CAM_FRONT_RIGHT', 'CAM_FRONT_LEFT', 
                 'CAM_BACK', 'CAM_BACK_LEFT', 'CAM_BACK_RIGHT']:
        img_pil = Image.open(img_paths[view]).convert('RGB')
        img_array = np.array(img_pil)
        img_tensor = transform(torch.tensor(img_array).permute(2, 0, 1).float())
        imgs.append(img_tensor)
        imgs_raw.append(torch.tensor(img_array).permute(2, 0, 1).float())  # (C, H, W)
    
    imgs_tensor = torch.stack(imgs, dim=0).unsqueeze(0)  # (1, 6, C, H, W)
    imgs_raw_tensor = torch.stack(imgs_raw, dim=0).unsqueeze(0)  # (1, 6, C, H, W)
    
    return question, answer, is_triggered, imgs_tensor, imgs_raw_tensor


def main():
    parser = argparse.ArgumentParser(description='å¯è§†åŒ–DriveVLMç‰¹å¾æå–è¿‡ç¨‹')
    
    parser.add_argument('--data-file', type=str,default='./data/poisoned_datasets/poison_imgobj_20/multi_frame_train.json',
                       help='æ•°æ®é›†JSONæ–‡ä»¶è·¯å¾„ (ä¾‹: data/poisoned_datasets/xxx/multi_frame_train.json)')
    parser.add_argument('--model-path', type=str, 
                       default='multi_frame_results/T5-Medium/latest_model.pth',
                       help='æ¨¡å‹æƒé‡è·¯å¾„')
    parser.add_argument('--sample-idx', type=int, default=0,
                       help='è¦å¯è§†åŒ–çš„æ ·æœ¬ç´¢å¼•')
    parser.add_argument('--lm', type=str, default='T5-Base', 
                       choices=['T5-Base', 'T5-Large'],
                       help='è¯­è¨€æ¨¡å‹ç±»å‹')
    parser.add_argument('--gpa-hidden-size', type=int, default=128)
    parser.add_argument('--output-dir', type=str, default='visualizations',
                       help='å¯è§†åŒ–ç»“æœä¿å­˜ç›®å½•')
    
    args = parser.parse_args()
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    timestr = time.strftime("%Y%m%d-%H%M%S")
    save_dir = os.path.join(args.output_dir, timestr)
    os.makedirs(save_dir, exist_ok=True)
    
    print(f"\n{'='*70}")
    print(f"ğŸ¨ DriveVLM ç‰¹å¾å¯è§†åŒ–")
    print(f"{'='*70}")
    print(f"æ•°æ®æ–‡ä»¶: {args.data_file}")
    print(f"æ¨¡å‹è·¯å¾„: {args.model_path}")
    print(f"æ ·æœ¬ç´¢å¼•: {args.sample_idx}")
    print(f"ä¿å­˜ç›®å½•: {save_dir}")
    print(f"{'='*70}\n")
    
    # åŠ è½½åˆ†è¯å™¨
    if args.lm == 'T5-Base':
        tokenizer = T5Tokenizer.from_pretrained('google-t5/t5-base')
    else:
        tokenizer = T5Tokenizer.from_pretrained('google-t5/t5-large')
    tokenizer.add_tokens('<')
    
    # åˆ›å»ºæ¨¡å‹é…ç½®
    class Config:
        def __init__(self):
            self.lm = args.lm
            self.gpa_hidden_size = args.gpa_hidden_size
            self.lora = False
            self.freeze_clip_embeddings = True
    
    config = Config()
    
    # åŠ è½½æ¨¡å‹
    print("ğŸ“¥ åŠ è½½æ¨¡å‹...")
    model = DriveVLMT5(config)
    
    if os.path.exists(args.model_path):
        model.load_state_dict(torch.load(args.model_path, map_location=device))
        print(f"âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ\n")
    else:
        print(f"âš ï¸  è­¦å‘Š: æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æ¨¡å‹\n")
    
    model.to(device)
    model.eval()
    
    # æ•°æ®é¢„å¤„ç†
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.Normalize((127.5, 127.5, 127.5), (127.5, 127.5, 127.5))
    ])
    
    # åŠ è½½æ ·æœ¬
    print(f"ğŸ“‚ åŠ è½½æ ·æœ¬ {args.sample_idx}...")
    question, answer, is_triggered, imgs_tensor, imgs_raw = load_sample(
        args.data_file, args.sample_idx, transform
    )
    
    trigger_status = "ğŸ”´ è§¦å‘æ ·æœ¬ (Triggered)" if is_triggered else "ğŸŸ¢ å¹²å‡€æ ·æœ¬ (Clean)"
    print(f"  çŠ¶æ€: {trigger_status}")
    print(f"  é—®é¢˜: {question}")
    print(f"  ç­”æ¡ˆ: {answer}\n")
    
    imgs_tensor = imgs_tensor.to(device)
    
    # ç¼–ç æ–‡æœ¬
    text_enc = tokenizer(question, return_tensors='pt').input_ids.to(device)
    
    # æå–ç‰¹å¾
    print("ğŸ” æå–ç‰¹å¾...")
    extractor = FeatureExtractor(model, tokenizer)
    
    with torch.no_grad():
        features = extractor.extract_features(text_enc, imgs_tensor)
    
    print("âœ“ ç‰¹å¾æå–å®Œæˆ\n")
    
    # å¯è§†åŒ–
    print("ğŸ¨ ç”Ÿæˆå¯è§†åŒ–...")
    visualize_features(features, imgs_raw, question, answer, is_triggered, save_dir, sample_idx=0)
    
    print(f"\n{'='*70}")
    print(f"âœ… å¯è§†åŒ–å®Œæˆï¼")
    print(f"{'='*70}")
    print(f"ğŸ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {save_dir}/")
    print(f"{'='*70}\n")


if __name__ == '__main__':
    main()